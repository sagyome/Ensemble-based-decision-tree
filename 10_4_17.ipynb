{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn import clone\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import StringIO\n",
    "import pydot\n",
    "import os\n",
    "import matplotlib\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import random\n",
    "DATASETS_PATH='/home/sagio/Unitree forest/datasets/'\n",
    "PICKLES_PATH='/home/sagio/Unitree forest/pickles/'\n",
    "class Branch:\n",
    "    def __init__(self,labels_probas=None,number_of_samples=None):\n",
    "        self.features_upper={k:np.inf for k in xrange(NUMBEROFFEATURES)}\n",
    "        self.features_lower={k:-np.inf for k in xrange(NUMBEROFFEATURES)}\n",
    "        if labels_probas is not None:\n",
    "            labels_probas=[x/np.sum(labels_probas) for x in labels_probas]\n",
    "            self.labels={k:v for k,v in zip(xrange(NUMOFLABELS),labels_probas)}\n",
    "            self.number_of_samples=number_of_samples\n",
    "    def addCondition(self,feature,threshold,bound):\n",
    "        if bound=='lower':\n",
    "            if self.features_lower[feature]<=threshold:\n",
    "                self.features_lower[feature]=np.round(threshold,2)+0.001\n",
    "        else:\n",
    "            if self.features_upper[feature]>=threshold:\n",
    "                self.features_upper[feature]=np.round(threshold,2)\n",
    "    def contradictBranch(self,b):\n",
    "        for i in xrange(NUMBEROFFEATURES):\n",
    "            if b.features_upper[i]<=self.features_lower[i] or b.features_lower[i]>=self.features_upper[i]:\n",
    "                return True\n",
    "        return False\n",
    "    def mergeBranch(self,b):\n",
    "        new_b=Branch()\n",
    "        new_b.features_upper,new_b.features_lower,new_b.labels=dict(self.features_upper),dict(self.features_lower),dict(self.labels)\n",
    "        for feature in xrange(NUMBEROFFEATURES):\n",
    "            new_b.addCondition(feature,b.features_upper[feature],'upper')\n",
    "            new_b.addCondition(feature,b.features_lower[feature],'lower')\n",
    "        new_b.labels={k:v1+v2 for k,v1,v2 in zip(xrange(NUMOFLABELS),new_b.labels.values(),b.labels.values())}\n",
    "        new_b.number_of_samples=np.sqrt(self.number_of_samples*b.number_of_samples)\n",
    "        return new_b\n",
    "    def toString(self):\n",
    "        s=\"\"\n",
    "        for feature,threshold in self.features_lower.iteritems():\n",
    "            if threshold!=(-np.inf):\n",
    "                s+='['+str(feature)+'] >'+str(threshold)+\", \"\n",
    "        for feature,threshold in self.features_upper.iteritems():\n",
    "            if threshold!=np.inf:\n",
    "                s+='['+str(feature)+'] <'+str(threshold)+\", \"\n",
    "        s+=str(self.labels)\n",
    "        return s\n",
    "    def printBranch(self):\n",
    "        print self.toString()\n",
    "    def getLabel(self):\n",
    "        return np.argmax(self.labels.values())\n",
    "    def containsInstance(self,v):\n",
    "        for index,item in enumerate(v):\n",
    "            if self.features_upper[index]<item or self.features_lower[index]>item:\n",
    "                return False\n",
    "        return True\n",
    "    def containsCondition(self,tup):\n",
    "        feature,threshold,bound=tup[0],tup[1],tup[2]\n",
    "        if bound=='upper':\n",
    "            if self.features_upper[feature]<=threshold:\n",
    "                return True\n",
    "        else:\n",
    "            if self.features_lower[feature]>=threshold:\n",
    "                return True\n",
    "        return False\n",
    "    def contradictCondition(self,tup):\n",
    "        feature,threshold,bound=tup[0],tup[1],tup[2]\n",
    "        if bound=='upper':\n",
    "            if self.features_lower[feature]>=threshold:\n",
    "                return True\n",
    "        else:\n",
    "            if self.features_upper[feature]<=threshold:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def get_branch(x,node_id):\n",
    "    b=Branch(x.value[node_id][0],x.n_node_samples[node_id])\n",
    "    while node_id!=0:\n",
    "        if node_id in x.children_left:\n",
    "            ancesor=np.where(x.children_left==node_id)[0][0]\n",
    "            b.addCondition(x.feature[ancesor],x.threshold[ancesor],'upper')\n",
    "            node_id=ancesor\n",
    "        else:\n",
    "            ancesor=np.where(x.children_right==node_id)[0][0]\n",
    "            b.addCondition(x.feature[ancesor],x.threshold[ancesor],'lower')\n",
    "            node_id=ancesor\n",
    "    return b\n",
    "def print_branches(branches):\n",
    "    for b in branches:\n",
    "        b.printBranch()\n",
    "branch_exp=re.compile('\\D+(?P<num>\\d+)\\] (?P<sign>\\D)')\n",
    "def opossite_direction(s):\n",
    "    print s\n",
    "    if s=='upper':\n",
    "        return 'lower'\n",
    "    else:\n",
    "        return 'upper'\n",
    "def feature_conversion(feature):\n",
    "    exp=re.compile('\\[x(?P<feature>[^\\]]+)\\]\\s(?P<sign>\\S)\\s(?P<threshold>.+)')\n",
    "    d=exp.match(f_names[feature]).groupdict()\n",
    "    d['bound']='lower' if d['sign']=='<' else 'upper'\n",
    "    del d['sign']\n",
    "    return d\n",
    "def string_conversion(s):\n",
    "    temp_d=branch_exp.match(s).groupdict()\n",
    "    num=int(temp_d['num'])\n",
    "    sign=temp_d['sign']\n",
    "    d=feature_conversion(num)\n",
    "    if sign=='>':\n",
    "        d['bound']=opposite_direction(d['bound'])\n",
    "    if d['bound']=='upper':\n",
    "        return 'x['+str(d['feature'])+'] '+'< '+str(d['threshold'])\n",
    "    else:\n",
    "        return 'x['+str(d['feature'])+'] '+'> '+str(d['threshold'])\n",
    "def get_route(x,node_id):\n",
    "    route=''\n",
    "    while node_id!=0:\n",
    "        if node_id in x.children_left:\n",
    "            route+='l'\n",
    "            node_id=np.where(x.children_left==node_id)[0][0]\n",
    "        else:\n",
    "            route+='r'\n",
    "            node_id=np.where(x.children_right==node_id)[0][0]\n",
    "    return route\n",
    "def branch_string_conversion(b):\n",
    "    new_s=''\n",
    "    for i in b.toString().split(\",\")[:-3]:\n",
    "        new_s+=string_conversion(i)+\", \"\n",
    "    return new_s\n",
    "def convert_string_to_cond(s):\n",
    "    feature=int(s.split(\"x\")[1].split(\"]\")[0])\n",
    "    value=float(s.split(\" \")[-1])\n",
    "    bound='upper' if \"<\" in s else 'lower'\n",
    "    return feature,value,bound\n",
    "def opposite_direction(s):\n",
    "    if s=='upper':\n",
    "        return 'lower'\n",
    "    else:\n",
    "        return 'upper'\n",
    "def fit_decision_tree_model(train_x,train_y):\n",
    "    parameters = {'criterion': ['entropy','gini'],\n",
    "                  'max_depth': [10,20,50],\n",
    "                  'min_samples_leaf': [1,2,5,10]}\n",
    "    model=DecisionTreeClassifier()\n",
    "    clfGS = GridSearchCV(model, parameters, cv=3)\n",
    "    clfGS.fit(train_x,train_y)\n",
    "    model=clfGS.best_estimator_\n",
    "    model.fit(train_x,train_y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_data_set():\n",
    "    data=pd.read_csv('breast-cancer-wisconsin.data',names=names)\n",
    "    data=data[data['Bare Nuclei']!='?']\n",
    "    data['Bare Nuclei']=[int(i) for i in data['Bare Nuclei']]\n",
    "    idx = np.arange(data.shape[0])\n",
    "    data.index=idx\n",
    "    X = data.drop(['class','code_number'],axis=1)\n",
    "    y = data['class']\n",
    "    return X,y\n",
    "def divide_to_train_test(X,y):\n",
    "    train_threshold=int(len(y)*0.7)\n",
    "    train_x=X[:train_threshold].as_matrix()\n",
    "    train_y=y[:train_threshold]\n",
    "    test_x=X[train_threshold:].as_matrix()\n",
    "    test_y=y[train_threshold:]\n",
    "    return train_x,train_y,test_x,test_y\n",
    "def create_output_dict(OUTPUT_PATH,train_x,train_y,test_x,test_y,ensemble_model,\n",
    "                       decision_tree_model,new_tree_model,comparison_df):\n",
    "    output_dict['train_X']=train_x\n",
    "    output_dict['train_Y']=train_y\n",
    "    output_dict['test_X']=test_x\n",
    "    output_dict['test_Y']=test_y\n",
    "    output_dict['ensemble_model']=ensemble_model\n",
    "    output_dict['decision_tree_model']=decision_tree_model\n",
    "    output_dict['ensemble_max_depth']=np.sum([x.tree_.max_depth for x in ensemble_model.estimators_])\n",
    "    output_dict['new_tree_max_depth']=new_tree_model.tree_.max_depth\n",
    "    output_dict['decision_tree_max_depth']=decision_tree_model.tree_.max_depth\n",
    "    output_dict['number_of_features']=NUMBEROFFEATURES\n",
    "    output_dict['number_of_labels']=NUMOFLABELS\n",
    "    output_dict['number_of_instances']=len(train_y)+len(test_y)\n",
    "    output_dict['ensemble_number_of_nodes']=np.sum([x.tree_.node_count for x in ensemble_model.estimators_])\n",
    "    output_dict['new_tree_number_of_nodes']=new_tree_model.tree_.node_count\n",
    "    output_dict['comparison_data_Set']=comparison_df\n",
    "    pickle.dump(output_dict,open(OUTPUT_PATH,'wb'))\n",
    "def fit_ensemble_model(train_x,train_y,n_estimators,max_depth=3,min_leaf_samples=10):\n",
    "    model=RandomForestClassifier(n_estimators=n_estimators,criterion='entropy',max_depth=max_depth,min_samples_leaf=min_leaf_samples)\n",
    "    model.fit(train_x,train_y)\n",
    "    return model\n",
    "def build_rules_set(ensemble_model):\n",
    "    x=ensemble_model.estimators_[0].tree_\n",
    "    leafs_indexes=[i for i in xrange(x.node_count) if x.children_left[i]==-1 and x.children_right[i]==-1]\n",
    "    branches=[get_branch(x,i) for i in leafs_indexes]\n",
    "    for idx in range(1,len(ensemble_model.estimators_)):\n",
    "        x=ensemble_model.estimators_[idx].tree_\n",
    "        leafs_indexes=[i for i in xrange(x.node_count) if x.children_left[i]==-1 and x.children_right[i]==-1]\n",
    "        temp_branches=[get_branch(x,i) for i in leafs_indexes]\n",
    "        branches1=[]\n",
    "        for b in branches:\n",
    "            for tb in temp_branches:\n",
    "                if b.contradictBranch(tb)==False:\n",
    "                    branches1.append(b.mergeBranch(tb))\n",
    "        branches=list(branches1)\n",
    "    print(\"Number of branches after iteration \"+str(idx)+\": \"+ str(len(branches)))\n",
    "    #print datetime.datetime.now()\n",
    "    return branches\n",
    "def get_branches_predictions(branches,test_x):\n",
    "    branches_predictions=[]\n",
    "    for i in xrange(len(test_x)):\n",
    "        found=0\n",
    "        for b in branches:\n",
    "            if b.containsInstance(test_x[i]):\n",
    "                found=1\n",
    "                branches_predictions.append(b.getLabel())\n",
    "                break\n",
    "        if found==0:\n",
    "            branches_predictions.append(None)\n",
    "    return branches_predictions\n",
    "def create_thresholds_vector_space(features_upper_thresholds,features_lower_thresholds):\n",
    "    upper_vectors={k:{} for k in features_upper_thresholds}\n",
    "    lower_vectors={k:{} for k in features_lower_thresholds}\n",
    "    for feature,threshold_values in features_upper_thresholds.iteritems():\n",
    "        for v in threshold_values:\n",
    "            upper_vectors[feature][v]={\"[x\"+str(feature)+\"] < \"+str(v1):(1 if v1 >= v else 0) for v1 in threshold_values}\n",
    "    for feature,threshold_values in features_lower_thresholds.iteritems():\n",
    "        for v in threshold_values:\n",
    "            lower_vectors[feature][v]={\"[x\"+str(feature)+\"] > \"+str(v1):(1 if v1 <= v else 0) for v1 in threshold_values}\n",
    "    return upper_vectors,lower_vectors\n",
    "def create_new_model_input(branches):\n",
    "    l=[]\n",
    "    features_upper_thresholds={k:set() for k in xrange(NUMBEROFFEATURES)}\n",
    "    features_lower_thresholds={k:set() for k in xrange(NUMBEROFFEATURES)}\n",
    "    for b in branches:\n",
    "        for k,v in b.features_upper.iteritems():\n",
    "            features_upper_thresholds[k].add(v)\n",
    "        for k,v in b.features_lower.iteritems():\n",
    "            features_lower_thresholds[k].add(v)\n",
    "    upper_vectors,lower_vectors=create_thresholds_vector_space(features_upper_thresholds,features_lower_thresholds)\n",
    "    for b in branches:\n",
    "        d={}\n",
    "        for k,v in b.features_upper.iteritems():\n",
    "            d.update(upper_vectors[k][v])\n",
    "        for k,v in b.features_lower.iteritems():\n",
    "            d.update(lower_vectors[k][v])\n",
    "        for k,v in b.labels.iteritems():\n",
    "            d['assigned_label']=k\n",
    "            d['weight']=v\n",
    "            l.append(dict(d))\n",
    "    ensemble_df=pd.DataFrame(l)\n",
    "    ensemble_df=ensemble_df.drop([col for col in ensemble_df.columns if '>' in col or 'inf' in col],axis=1)\n",
    "    return ensemble_df\n",
    "\n",
    "def create_new_test_set(test_x,new_model_input_data):\n",
    "    new_tree_test_x=[]\n",
    "    for i in test_x:\n",
    "        transformed=[]\n",
    "        for col in [col for col in new_model_input_data.columns if '[x' in col.lower()]:\n",
    "            feature=int(col.split(\"x\")[1].split(\"]\")[0])\n",
    "            value=float(col.split(\" \")[2])\n",
    "            is_upper=1 if (col.split(\" \")[1])=='<' else 0\n",
    "            if (is_upper and i[feature]<=value) or (is_upper==0 and i[feature]>=value):\n",
    "                transformed.append(1)\n",
    "            else:\n",
    "                transformed.append(0)\n",
    "        new_tree_test_x.append(transformed)\n",
    "    new_tree_test_x=np.array(new_tree_test_x)\n",
    "    return new_tree_test_x\n",
    "def fit_new_model(new_model_input_data):\n",
    "    new_tree_model=DecisionTreeClassifier(criterion='entropy',min_samples_split=1)\n",
    "    f_names=[col for col in new_model_input_data.columns if '[x' in col.lower()]\n",
    "    X=new_model_input_data[f_names].as_matrix()\n",
    "    y=new_model_input_data['assigned_label']\n",
    "    new_tree_model.fit(X,y,sample_weight=new_model_input_data['weight'].values)\n",
    "    return new_tree_model\n",
    "def create_output_df(test_x,test_y,ensemble_model,new_tree_model,decision_tree_model,new_model_input_data,branches_predictions):\n",
    "    comparison_df=pd.DataFrame(test_x)\n",
    "    comparison_df['ensmble_predictions']=ensemble_model.predict(test_x)\n",
    "    new_model_predictions,new_model_depth,new_model_probas = get_new_model_predictions(new_tree_model,test_x,new_model_input_data)\n",
    "    output_dict['new_model_probas']=np.array([list(i) for i in new_model_probas])\n",
    "    output_dict['ensemble_probas']=ensemble_model.predict_proba(test_x)\n",
    "    comparison_df['new_tree_predictions']=new_model_predictions\n",
    "    comparison_df['new_model_depth']=new_model_depth\n",
    "    output_dict['decision_tree_depth']=np.log2(len(decision_tree_model.tree_.n_node_samples))\n",
    "    comparison_df['actual']=list(test_y)\n",
    "    if len(branches_predictions)==len(comparison_df):\n",
    "        comparison_df['branches_predictions']=branches_predictions\n",
    "    comparison_df['decision_tree_predictions']=decision_tree_model.predict(test_x)\n",
    "    instances_depth=[]\n",
    "    new_model_depth=[]\n",
    "    for i in test_x:\n",
    "        temp_depth=[]\n",
    "        for m in ensemble_model.estimators_:\n",
    "            temp_depth.append(np.round(np.log2(m.apply(i)[0])))\n",
    "        instances_depth.append(np.sum(temp_depth))\n",
    "    comparison_df['ensemble_depth']=instances_depth\n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tree conversion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_new_features_and_new_thresholds(t,new_model_input_data):\n",
    "    f_names=[col for col in new_model_input_data.columns if 'x' in col]\n",
    "    new_features=[]\n",
    "    new_thresholds=[]\n",
    "    for feature in t.feature:\n",
    "        if feature==-2:\n",
    "            new_features.append(feature)\n",
    "            new_thresholds.append(-2)\n",
    "        else:\n",
    "            new_f,new_t=get_new_node(feature,f_names)\n",
    "            new_features.append(new_f)\n",
    "            new_thresholds.append(new_t)\n",
    "    return new_features,new_thresholds\n",
    "def get_new_model_predictions(new_model,test_x,new_model_input_data):\n",
    "    new_model_predictions=[]\n",
    "    new_model_depth=[]\n",
    "    new_model_probas=[]\n",
    "    t=new_model.tree_\n",
    "    new_features,new_thresholds=get_new_features_and_new_thresholds(t,new_model_input_data)\n",
    "    for inst in test_x:\n",
    "        d=0\n",
    "        i=0\n",
    "        while new_features[i]!=-2:\n",
    "            d+=1\n",
    "            if inst[new_features[i]]>new_thresholds[i]:\n",
    "                i=t.children_left[i]\n",
    "            else:\n",
    "                i=t.children_right[i]\n",
    "        new_model_depth.append(d)\n",
    "        new_model_predictions.append(np.argmax(t.value[i]))\n",
    "        new_model_probas.append(t.value[i][0])\n",
    "    #new_model_predictions=[4 if i else 2 for i in new_model_predictions]\n",
    "    return new_model_predictions,new_model_depth,new_model_probas\n",
    "def get_new_node(feature,f_names):\n",
    "    new_f=int(f_names[feature].split(\"x\")[1].split(']')[0])\n",
    "    new_t=float(f_names[feature].split(' ')[2])\n",
    "    return new_f,new_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_data_set(s):\n",
    "    if s=='breast_cancer':\n",
    "        names=['code_number','Clump_thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size'\n",
    "            ,'Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','class']\n",
    "        data=pd.read_csv(DATASETS_PATH+'breast-cancer-wisconsin.data',names=names)\n",
    "        data=data[data['Bare Nuclei']!='?']\n",
    "        data['Bare Nuclei']=[int(i) for i in data['Bare Nuclei']]\n",
    "        data['class']=[0 if i==2 else 1 for i in data['class']]\n",
    "    if s=='iris':\n",
    "        iris = load_iris()\n",
    "        data = pd.DataFrame(iris.data[:],columns=iris.feature_names)\n",
    "        data['class'] = iris.target\n",
    "    if s=='winery':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"wine.data\",names=['class','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium',\n",
    "                                     'Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins',\n",
    "                                    'Color intensity','Hue','OD280/OD315 of diluted wines','Proline'])\n",
    "        data['class']=[i-1 for i in data['class']]\n",
    "    if s=='vehicle':\n",
    "        column_names=[str(i) for i in xrange(19)]\n",
    "        column_names.append('class')\n",
    "        pathes=glob.glob(DATASETS_PATH+\"vehicle_data/xa*.dat\")\n",
    "        data=pd.read_csv(pathes[0],sep=\" \",names=column_names)\n",
    "        for p in pathes[1:]:\n",
    "            data=data.append(pd.read_csv(p,sep=\" \",names=column_names),ignore_index=True)\n",
    "        data['class']=data['18']\n",
    "        data=data.drop(['18'],axis=1)\n",
    "        new_classes={v:k for k,v in enumerate(list(set(data['class'].values)))}\n",
    "        data['class']=[new_classes[i] for i in data['class']]\n",
    "    if s=='car':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"car.data\",names=['buying','maint','doors','persons','lug_boot','safety','class'])\n",
    "        for col in data.columns[:-1]:\n",
    "            temp_df=pd.get_dummies(data[col])\n",
    "            temp_df.columns=[col+\"_\"+val for val in temp_df.columns]\n",
    "            data=data.join(temp_df)\n",
    "            data=data.drop([col],axis=1)\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='glass':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"glass.data\",names=['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe','class'])\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='aust_credit':\n",
    "        names=[\"A\"+str(i) for i in range(1,15)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(DATASETS_PATH+\"australian.dat\",sep=\" \",names=names)\n",
    "    if s=='nurse':\n",
    "        names=['x'+str(i) for i in range(1,9)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(DATASETS_PATH+\"post-operative.data\",names=names)\n",
    "        for col in data.columns[:-1]:\n",
    "            temp_df=pd.get_dummies(data[col])\n",
    "            temp_df.columns=[col+\"_\"+val for val in temp_df.columns]\n",
    "            data=data.join(temp_df)\n",
    "            data=data.drop([col],axis=1)\n",
    "        new_classes={v:k for k,v in enumerate(list(set(data['class'].values)))}\n",
    "        data['class']=[new_classes[i] for i in data['class']]\n",
    "    if s=='diabetes':\n",
    "        names=['x'+str(i) for i in range(1,9)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(DATASETS_PATH+\"pima-indians-diabetes.data\",names=names)\n",
    "    if s=='monk1':\n",
    "        names=['class']\n",
    "        names.extend(['x'+str(i) for i in range(1,8)])\n",
    "        data=pd.read_csv(DATASETS_PATH+\"monks-1.train\",sep=\" \",names=names)\n",
    "        data=data.append(pd.read_csv(DATASETS_PATH+\"monks-1.test\",sep=\" \",names=names))\n",
    "        data.index=np.arange(len(data))\n",
    "        data=data.drop(['x7'],axis=1)\n",
    "    if s=='monk2':\n",
    "        names=['class']\n",
    "        names.extend(['x'+str(i) for i in range(1,8)])\n",
    "        data=pd.read_csv(DATASETS_PATH+\"monks-2.train\",sep=\" \",names=names)\n",
    "        data=data.append(pd.read_csv(DATASETS_PATH+\"monks-2.test\",sep=\" \",names=names))\n",
    "        data.index=np.arange(len(data))\n",
    "        data=data.drop(['x7'],axis=1)\n",
    "    if s=='monk3':\n",
    "        names=['class']\n",
    "        names.extend(['x'+str(i) for i in range(1,8)])\n",
    "        data=pd.read_csv(DATASETS_PATH+\"monks-3.train\",sep=\" \",names=names)\n",
    "        data=data.append(pd.read_csv(DATASETS_PATH+\"monks-3.test\",sep=\" \",names=names))\n",
    "        data.index=np.arange(len(data))\n",
    "        data=data.drop(['x7'],axis=1)\n",
    "    if s=='zoo':\n",
    "        names=['x'+str(i) for i in range(0,17)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(DATASETS_PATH+'zoo.data',names=names)\n",
    "        data=data.drop(['x0'],axis=1)\n",
    "        data['class']=[i-1 for i in data['class']]\n",
    "    \n",
    "    if s=='tic_tac_toe':\n",
    "        names=[\"x\"+str(i) for i in range(1,10)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(\"/home/sagio/Unitree forest/datasets/tic-tac-toe.data\",names=names)\n",
    "        for col in data.columns[:-1]:\n",
    "            temp_df=pd.get_dummies(data[col])\n",
    "            temp_df.columns=[col+\"_\"+val for val in temp_df.columns]\n",
    "            data=data.join(temp_df)\n",
    "            data=data.drop([col],axis=1)\n",
    "        data['class']=[1 if i=='positive' else 0 for i in data['class']]\n",
    "    if s=='letter':\n",
    "        names=['class']\n",
    "        names.extend(['x'+str(i) for i in xrange(1,17)])\n",
    "        data=pd.read_csv(DATASETS_PATH+\"letter-recognition.data\",names=names)\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='balance_scale':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"balance-scale.data\",names=['class','x1','x2','x3','x4'])\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='ecoli':\n",
    "        f=open(DATASETS_PATH+\"ecoli.data\")\n",
    "        line=f.readline()\n",
    "        l=[]\n",
    "        names=['x'+str(i) for i in range(1,9)]\n",
    "        names.append('class')\n",
    "        while line:\n",
    "            line=line.replace(\"\\n\",\"\").replace(\"    \",\"   \").replace(\"   \",\"  \").replace(\"  \",\" \").split(\" \")\n",
    "            l.append({k:v for k,v in zip(names,line)})\n",
    "            line=f.readline()\n",
    "        data=pd.DataFrame(l)\n",
    "        data=data.drop(['x1'],axis=1)\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='transfusion':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"transfusion.data\")\n",
    "        data['class']=data['whether he/she donated blood in March 2007']\n",
    "        data=data.drop(['whether he/she donated blood in March 2007'], axis=1)\n",
    "    if s=='user_modelling':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"User_Modeling.csv\")\n",
    "        data['class']=data[' UNS']\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "        data=data.drop([' UNS'],axis=1)\n",
    "    if s=='kohkiloyeh':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"kohkiloyeh.csv\")\n",
    "        data['class']=data['pb']\n",
    "        data=data.drop(['pb'],axis=1)\n",
    "        for col in data.columns[:-1]:\n",
    "            temp_df=pd.get_dummies(data[col])\n",
    "            temp_df.columns=[col+\"_\"+val for val in temp_df.columns]\n",
    "            data=data.join(temp_df)\n",
    "            data=data.drop([col],axis=1)\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='haberman':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"haberman.data\",names=['x1','x2','x3','class'])\n",
    "        data['class']=[i-1 for i in data['class']]\n",
    "    data=data.sample(frac=1)\n",
    "    X = data.drop(['class'],axis=1)\n",
    "    y = data['class'].values\n",
    "    return X,y\n",
    "def run_experiment(s,number_of_trees,day_string,max_depth=3,min_leaf_samples=10):\n",
    "    for RANDOM_SEED in range(1,100):\n",
    "        global output_dict\n",
    "        output_dict={}\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        dest=PICKLES_PATH+s+\"/\"+day_string+\"_\"+str(number_of_trees)+\"trees\"\n",
    "        if not os.path.isdir(dest):\n",
    "            os.makedirs(dest)\n",
    "        OUTPUT_PATH=dest+\"/seed_\"+str(RANDOM_SEED)+\".pkl\"\n",
    "        if os.path.isfile(OUTPUT_PATH):\n",
    "            continue\n",
    "        X,y=create_data_set(s)\n",
    "        global NUMOFLABELS\n",
    "        global NUMBEROFFEATURES\n",
    "        NUMOFLABELS=len(set(y))\n",
    "        NUMBEROFFEATURES=len(X.columns)\n",
    "        train_x,train_y,test_x,test_y=divide_to_train_test(X,y)\n",
    "        ensemble_model=fit_ensemble_model(train_x,train_y,number_of_trees,max_depth=max_depth,min_leaf_samples=min_leaf_samples)\n",
    "        decision_tree_model=fit_decision_tree_model(train_x,train_y)\n",
    "        branches=build_rules_set(ensemble_model)\n",
    "        branches_predictions=get_branches_predictions(branches,test_x)\n",
    "        new_model_input_data=create_new_model_input(branches)\n",
    "        new_tree_model=fit_new_model(new_model_input_data)\n",
    "        new_tree_test_x=create_new_test_set(test_x,new_model_input_data)\n",
    "        comparison_df=create_output_df(test_x,test_y,ensemble_model,new_tree_model,decision_tree_model,new_model_input_data,branches_predictions)\n",
    "        create_output_dict(OUTPUT_PATH,train_x,train_y,test_x,test_y,ensemble_model,\n",
    "                       decision_tree_model,new_tree_model,comparison_df)\n",
    "        print float(len(comparison_df[comparison_df['actual']==comparison_df['ensmble_predictions']]))/len(comparison_df)\n",
    "        print float(len(comparison_df[comparison_df['actual']==comparison_df['new_tree_predictions']]))/len(comparison_df)\n",
    "        print float(len(comparison_df[comparison_df['actual']==comparison_df['decision_tree_predictions']]))/len(comparison_df)\n",
    "day_string='10_4_17'\n",
    "run_experiment('iris',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('iris',20,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('breast_cancer',7,day_string)\n",
    "run_experiment('breast_cancer',10,day_string)\n",
    "run_experiment('winery',7,day_string)\n",
    "run_experiment('winery',10,day_string)\n",
    "run_experiment('aust_credit',7,day_string)\n",
    "run_experiment('aust_credit',10,day_string)\n",
    "run_experiment('nurse',7,day_string)\n",
    "run_experiment('nurse',10,day_string)\n",
    "run_experiment('diabetes',7,day_string)\n",
    "run_experiment('diabetes',10,day_string)\n",
    "run_experiment('zoo',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('zoo',20,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('balance_scale',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('balance_scale',20,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('transfusion',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('transfusion',20,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('kohkiloyeh',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('kohkiloyeh',7,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('haberman',7,day_string)\n",
    "run_experiment('haberman',10,day_string)\n",
    "run_experiment('user_modelling',10,day_string)\n",
    "run_experiment('user_modelling',20,day_string,max_depth=10,min_leaf_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiment('transfusion',20,day_string,max_depth=10,min_leaf_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f=open(DATASETS_PATH+\"ecoli.data\")\n",
    "line=f.readline()\n",
    "l=[]\n",
    "names=['x'+str(i) for i in range(1,9)]\n",
    "names.append('class')\n",
    "while line:\n",
    "    if line.startswith(\" \"):\n",
    "        line=\"\".join(line[1:])\n",
    "    line=line.replace(\"\\n\",\"\").replace(\"    \",\"   \").replace(\"   \",\"  \").replace(\"  \",\" \").split(\" \")\n",
    "    l.append({k:v for k,v in zip(names,line)})\n",
    "    line=f.readline()\n",
    "data=pd.DataFrame(l)\n",
    "data=data.drop(['x1'],axis=1)\n",
    "class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "data['class']=[class_map[i] for i in data['class']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(DATASETS_PATH+\"transfusion.data\")\n",
    "data['class']=data['whether he/she donated blood in March 2007']\n",
    "data=data.drop(['whether he/she donated blood in March 2007'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(DATASETS_PATH+\"kohkiloyeh.csv\")\n",
    "data['class']=data['pb']\n",
    "data=data.drop(['pb'],axis=1)\n",
    "for col in data.columns[:-1]:\n",
    "    temp_df=pd.get_dummies(data[col])\n",
    "    temp_df.columns=[col+\"_\"+val for val in temp_df.columns]\n",
    "    data=data.join(temp_df)\n",
    "    data=data.drop([col],axis=1)\n",
    "class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "data['class']=[class_map[i] for i in data['class']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "names=['class']\n",
    "names.extend(['x'+str(i) for i in range(1,8)])\n",
    "data=pd.read_csv(DATASETS_PATH+\"monks-2.train\",sep=\" \",names=names)\n",
    "data=data.append(pd.read_csv(DATASETS_PATH+\"monks-2.test\",sep=\" \",names=names))\n",
    "data.index=np.arange(len(data))\n",
    "data=data.drop(['x7'],axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "number_of_trees=7\n",
    "ensemble_model=fit_ensemble_model(train_x,train_y,number_of_trees)\n",
    "decision_tree_model=fit_decision_tree_model(train_x,train_y)\n",
    "branches=build_rules_set(ensemble_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in test_x:\n",
    "    for b in branches:\n",
    "        if b.containsInstance(i):\n",
    "            l.append(b.getLabel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
