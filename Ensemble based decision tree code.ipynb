{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Relevant files paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DATASETS_PATH='/home/sagio/Unitree forest/datasets/'\n",
    "PICKLES_PATH='/home/sagio/Unitree forest/pickles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Branch class\n",
    "A branch represent the concept of a rules-set that result in a classes vector output. The output of the first stage of our method is a set of branches instances.\n",
    "\n",
    "\n",
    "**Important note:** Conditions are implemented as features threshold, each threshold type (upper and lower) has a vector v so |v| equals Number of features. conditions are added as features thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Branch:\n",
    "    def __init__(self,labels_probas=None,number_of_samples=None):\n",
    "        \"\"\"Branch inatance can be initialized in 2 ways. One option is to initialize an empty branch\n",
    "        (only with a global number of features and number of class labels) and gradually add \n",
    "        conditions - this option is relevant for the merge implementation.\n",
    "        Second option is to get the number of samples in branch and the labels\n",
    "        probability vector - relevant for creating a branch out of an existing tree leaf.\n",
    "        \"\"\"\n",
    "        self.features_upper={k:np.inf for k in xrange(NUMBEROFFEATURES)} #upper bound of the feature for the given rule\n",
    "        self.features_lower={k:-np.inf for k in xrange(NUMBEROFFEATURES)} #lower bound of the feature for the given rule\n",
    "        if labels_probas is not None:\n",
    "            labels_probas=[x/np.sum(labels_probas) for x in labels_probas] #set the result vector\n",
    "            self.labels={k:v for k,v in zip(xrange(NUMOFLABELS),labels_probas)}\n",
    "            self.number_of_samples=number_of_samples #save number of samples in leaf (not relevant for the current model)\n",
    "    def addCondition(self,feature,threshold,bound):\n",
    "        \"\"\"\n",
    "        This function gets feature index, its threshold for the condition and whether\n",
    "        it is upper or lower bound. It updates the features thresholds for the given rule.\n",
    "        \"\"\"\n",
    "        if bound=='lower':\n",
    "            if self.features_lower[feature]<=threshold:\n",
    "                self.features_lower[feature]=np.round(threshold,2)+0.001\n",
    "        else:\n",
    "            if self.features_upper[feature]>=threshold:\n",
    "                self.features_upper[feature]=np.round(threshold,2)\n",
    "    def contradictBranch(self,b):\n",
    "        \"\"\"\n",
    "        check wether Branch b can be merged with the \"self\" Branch. Returns Boolean answer.\n",
    "        \"\"\"\n",
    "        for i in xrange(NUMBEROFFEATURES):\n",
    "            if b.features_upper[i]<=self.features_lower[i] or b.features_lower[i]>=self.features_upper[i]:\n",
    "                return True\n",
    "        return False\n",
    "    def mergeBranch(self,b):\n",
    "        \"\"\"\n",
    "        This method gets Branch b and create a new branch which is a merge of the \"self\" object\n",
    "        with b. As describe in the algorithm.\n",
    "        \"\"\"\n",
    "        new_b=Branch()\n",
    "        new_b.features_upper,new_b.features_lower,new_b.labels=dict(self.features_upper),dict(self.features_lower),dict(self.labels)\n",
    "        for feature in xrange(NUMBEROFFEATURES):\n",
    "            new_b.addCondition(feature,b.features_upper[feature],'upper')\n",
    "            new_b.addCondition(feature,b.features_lower[feature],'lower')\n",
    "        new_b.labels={k:v1+v2 for k,v1,v2 in zip(xrange(NUMOFLABELS),new_b.labels.values(),b.labels.values())}\n",
    "        new_b.number_of_samples=np.sqrt(self.number_of_samples*b.number_of_samples)\n",
    "        return new_b\n",
    "    def toString(self):\n",
    "        \"\"\"\n",
    "        This function creates a string representation of the branch (only for demonstration purposes)\n",
    "        \"\"\"\n",
    "        s=\"\"\n",
    "        for feature,threshold in self.features_lower.iteritems():\n",
    "            if threshold!=(-np.inf):\n",
    "                s+='['+str(feature)+'] >'+str(threshold)+\", \"\n",
    "        for feature,threshold in self.features_upper.iteritems():\n",
    "            if threshold!=np.inf:\n",
    "                s+='['+str(feature)+'] <'+str(threshold)+\", \"\n",
    "        s+=str(self.labels)\n",
    "        return s\n",
    "    def printBranch(self):\n",
    "        #print the branch by using tostring()\n",
    "        print self.toString()\n",
    "    def getLabel(self):\n",
    "        #Return the predicted label accordint to the branch\n",
    "        return np.argmax(self.labels.values())\n",
    "    def containsInstance(self,v):\n",
    "        \"\"\"This function gets an ibservation as an input. It returns True if the set of rules\n",
    "        that represented by the branch matches the instance and false otherwise.\n",
    "        \"\"\"\n",
    "        for index,item in enumerate(v):\n",
    "            if self.features_upper[index]<item or self.features_lower[index]>item:\n",
    "                return False\n",
    "        return True\n",
    "    def containsCondition(self,tup):\n",
    "        \"\"\"\n",
    "        This function gets a three items tuple with the following structure: (feature index, threshold,\n",
    "        upper or lower bound). The tuple represent a rule. Function returns True if this rule already\n",
    "        contained in the branch and False otherwise.\n",
    "        \"\"\"\n",
    "        feature,threshold,bound=tup[0],tup[1],tup[2]\n",
    "        if bound=='upper':\n",
    "            if self.features_upper[feature]<=threshold:\n",
    "                return True\n",
    "        else:\n",
    "            if self.features_lower[feature]>=threshold:\n",
    "                return True\n",
    "        return False\n",
    "    def contradictCondition(self,tup):\n",
    "        \"\"\"\n",
    "        An helping function for contradictBranch. Gets a tuple (same as in containsCondition), returns\n",
    "        True if this condition cannot be merged within the branch.\n",
    "        \"\"\"\n",
    "        feature,threshold,bound=tup[0],tup[1],tup[2]\n",
    "        if bound=='upper':\n",
    "            if self.features_lower[feature]>=threshold:\n",
    "                return True\n",
    "        else:\n",
    "            if self.features_upper[feature]<=threshold:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: functions for building the rules-sets\n",
    "The following functions ensble the execution of the first stage algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_branch(x,node_id):\n",
    "    \"\"\"\n",
    "    This function gets an sklearn tree x and a node_id of a leaf.\n",
    "    It creates a branch object for the given leaf in the tree\n",
    "    \"\"\"\n",
    "    b=Branch(x.value[node_id][0],x.n_node_samples[node_id])\n",
    "    while node_id!=0:\n",
    "        if node_id in x.children_left:\n",
    "            ancesor=np.where(x.children_left==node_id)[0][0]\n",
    "            b.addCondition(x.feature[ancesor],x.threshold[ancesor],'upper')\n",
    "            node_id=ancesor\n",
    "        else:\n",
    "            ancesor=np.where(x.children_right==node_id)[0][0]\n",
    "            b.addCondition(x.feature[ancesor],x.threshold[ancesor],'lower')\n",
    "            node_id=ancesor\n",
    "    return b\n",
    "def print_branches(branches):\n",
    "    #print a set of branches (denoted as rules-sets in the method description)\n",
    "    for b in branches:\n",
    "        b.printBranch()\n",
    "branch_exp=re.compile('\\D+(?P<num>\\d+)\\] (?P<sign>\\D)')\n",
    "def fit_decision_tree_model(train_x,train_y):\n",
    "    \"\"\"\n",
    "    This function gets train data and conducts a gridsearch for the best decision tree\n",
    "    out of several options. It returns the fitted tree\n",
    "    \"\"\"\n",
    "    parameters = {'criterion': ['entropy','gini'],\n",
    "                  'max_depth': [10,20,50],\n",
    "                  'min_samples_leaf': [1,2,5,10]}\n",
    "    model=DecisionTreeClassifier()\n",
    "    clfGS = GridSearchCV(model, parameters, cv=3)\n",
    "    clfGS.fit(train_x,train_y)\n",
    "    model=clfGS.best_estimator_\n",
    "    model.fit(train_x,train_y)\n",
    "    return model\n",
    "def divide_to_train_test(X,y):\n",
    "    #Divide X and y to train and test sets\n",
    "    train_threshold=int(len(y)*0.7)\n",
    "    train_x=X[:train_threshold].as_matrix()\n",
    "    train_y=y[:train_threshold]\n",
    "    test_x=X[train_threshold:].as_matrix()\n",
    "    test_y=y[train_threshold:]\n",
    "    return train_x,train_y,test_x,test_y\n",
    "def create_output_dict(OUTPUT_PATH,train_x,train_y,test_x,test_y,ensemble_model,\n",
    "                       decision_tree_model,new_tree_model,comparison_df):\n",
    "    \"\"\"\"\n",
    "    This function gets results of a single experiment. It stores the results in a dictionay and\n",
    "    saves that dictionary as a pickle file for serialization.\n",
    "    \"\"\"\n",
    "    output_dict['train_X']=train_x\n",
    "    output_dict['train_Y']=train_y\n",
    "    output_dict['test_X']=test_x\n",
    "    output_dict['test_Y']=test_y\n",
    "    output_dict['ensemble_model']=ensemble_model\n",
    "    output_dict['decision_tree_model']=decision_tree_model\n",
    "    output_dict['ensemble_max_depth']=np.sum([x.tree_.max_depth for x in ensemble_model.estimators_])\n",
    "    output_dict['new_tree_max_depth']=new_tree_model.tree_.max_depth\n",
    "    output_dict['decision_tree_max_depth']=decision_tree_model.tree_.max_depth\n",
    "    output_dict['number_of_features']=NUMBEROFFEATURES\n",
    "    output_dict['number_of_labels']=NUMOFLABELS\n",
    "    output_dict['number_of_instances']=len(train_y)+len(test_y)\n",
    "    output_dict['ensemble_number_of_nodes']=np.sum([x.tree_.node_count for x in ensemble_model.estimators_])\n",
    "    output_dict['new_tree_number_of_nodes']=new_tree_model.tree_.node_count\n",
    "    output_dict['comparison_data_Set']=comparison_df\n",
    "    pickle.dump(output_dict,open(OUTPUT_PATH,'wb'))\n",
    "def fit_ensemble_model(train_x,train_y,n_estimators,max_depth=3,min_leaf_samples=10):\n",
    "    #Fit an ensemble model for the given dataset\n",
    "    model=RandomForestClassifier(n_estimators=n_estimators,criterion='entropy',max_depth=max_depth,min_samples_leaf=min_leaf_samples)\n",
    "    model.fit(train_x,train_y)\n",
    "    return model\n",
    "def build_rules_set(ensemble_model):\n",
    "    \"\"\"\n",
    "    This is an implementation of the first stage of our method, here we create \n",
    "    the rules-sets out of the given ensemble.\n",
    "    \"\"\"\n",
    "    x=ensemble_model.estimators_[0].tree_ #extract the first tree\n",
    "    leafs_indexes=[i for i in xrange(x.node_count) if x.children_left[i]==-1 and x.children_right[i]==-1]\n",
    "    branches=[get_branch(x,i) for i in leafs_indexes]#Create branch object for each of the leaves\n",
    "    for idx in range(1,len(ensemble_model.estimators_)): #Iterate over the trees\n",
    "        x=ensemble_model.estimators_[idx].tree_\n",
    "        leafs_indexes=[i for i in xrange(x.node_count) if x.children_left[i]==-1 and x.children_right[i]==-1]\n",
    "        temp_branches=[get_branch(x,i) for i in leafs_indexes] #current tree leaves\\branches\n",
    "        branches1=[] #initialize the branches list for the current iteration\n",
    "        for b in branches: #iterate over each branch from last phase\n",
    "            for tb in temp_branches: #for each leaf in current tree\n",
    "                if b.contradictBranch(tb)==False: #check if the tree leaf can be merged with the branch \n",
    "                    branches1.append(b.mergeBranch(tb)) #if can be merged, add the merged branches to the rules-set\n",
    "        branches=list(branches1)\n",
    "    print(\"Total number of branches (rules-sets): \" + str(len(branches)))\n",
    "    #print datetime.datetime.now()\n",
    "    return branches\n",
    "def get_branches_predictions(branches,test_x):\n",
    "    \"\"\"\n",
    "    This function enables the evaluation of the rerieved rules-setrs\\branches list.\n",
    "    It gets a list of branches and a test set as an input and returns the branches predictions\n",
    "    for the given test set. predictions are made here by iterate over the rules sets and find\n",
    "    the matching rule for each instance.\n",
    "    \"\"\"\n",
    "    branches_predictions=[]\n",
    "    for i in xrange(len(test_x)):\n",
    "        found=0\n",
    "        for b in branches:\n",
    "            if b.containsInstance(test_x[i]):\n",
    "                found=1\n",
    "                branches_predictions.append(b.getLabel())\n",
    "                break\n",
    "        if found==0:\n",
    "            branches_predictions.append(None)\n",
    "    return branches_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Functions for building the trees out of the rules sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_thresholds_vector_space(features_upper_thresholds,features_lower_thresholds):\n",
    "    \"\"\"\n",
    "    Helping function for generating the new model dataset. Gets all branches thresholds\n",
    "    and creates the features-space as a set of all relevant columns (except from weight and class)\n",
    "    \"\"\"\n",
    "    upper_vectors={k:{} for k in features_upper_thresholds}\n",
    "    lower_vectors={k:{} for k in features_lower_thresholds}\n",
    "    for feature,threshold_values in features_upper_thresholds.iteritems():\n",
    "        for v in threshold_values:\n",
    "            upper_vectors[feature][v]={\"[x\"+str(feature)+\"] < \"+str(v1):(1 if v1 >= v else 0) for v1 in threshold_values}\n",
    "    for feature,threshold_values in features_lower_thresholds.iteritems():\n",
    "        for v in threshold_values:\n",
    "            lower_vectors[feature][v]={\"[x\"+str(feature)+\"] > \"+str(v1):(1 if v1 <= v else 0) for v1 in threshold_values}\n",
    "    return upper_vectors,lower_vectors\n",
    "def create_new_model_input(branches):\n",
    "    \"\"\"\n",
    "    Create the new model dataset out of the given rules-sets (branches)\n",
    "    \"\"\"\n",
    "    l=[] #This list will include all the records\n",
    "    features_upper_thresholds={k:set() for k in xrange(NUMBEROFFEATURES)} #possible upper thresholds for each feature\n",
    "    features_lower_thresholds={k:set() for k in xrange(NUMBEROFFEATURES)} #possible lower thresholds for each feature\n",
    "    for b in branches: #This loop updates the features space according to the given features in each branch\n",
    "        for k,v in b.features_upper.iteritems():\n",
    "            features_upper_thresholds[k].add(v)\n",
    "        for k,v in b.features_lower.iteritems():\n",
    "            features_lower_thresholds[k].add(v)\n",
    "    #upper_vectors and lower_vectors maps an input of feature and value into its relevant vector\n",
    "    #For example: x1< 2 will return 1 for (x1<2) and also 1 for (x1<3)\n",
    "    upper_vectors,lower_vectors=create_thresholds_vector_space(features_upper_thresholds,features_lower_thresholds)\n",
    "    for b in branches:\n",
    "        d={} #d represents a record within the dataset\n",
    "        for k,v in b.features_upper.iteritems():\n",
    "            d.update(upper_vectors[k][v]) \n",
    "        for k,v in b.features_lower.iteritems():\n",
    "            d.update(lower_vectors[k][v])\n",
    "        for k,v in b.labels.iteritems():\n",
    "            d['assigned_label']=k\n",
    "            d['weight']=v\n",
    "            l.append(dict(d)) #Notice!!! we create a record for every possible class butwith different weights\n",
    "    ensemble_df=pd.DataFrame(l) #greate the dataset and then remove upper bound features (it's redundant to the lower bounds)\n",
    "    #and also remove the infiniy thresholds features\n",
    "    ensemble_df=ensemble_df.drop([col for col in ensemble_df.columns if '>' in col or 'inf' in col],axis=1)\n",
    "    return ensemble_df\n",
    "def fit_new_model(new_model_input_data):\n",
    "    \"\"\"\n",
    "    Function gets the new model input data and fits an sklearn decision tree which is returned\n",
    "    to the user\n",
    "    \"\"\"\n",
    "    new_tree_model=DecisionTreeClassifier(criterion='entropy',min_samples_split=1)\n",
    "    f_names=[col for col in new_model_input_data.columns if '[x' in col.lower()]\n",
    "    X=new_model_input_data[f_names].as_matrix()\n",
    "    y=new_model_input_data['assigned_label']\n",
    "    new_tree_model.fit(X,y,sample_weight=new_model_input_data['weight'].values)\n",
    "    return new_tree_model\n",
    "def create_output_df(test_x,test_y,ensemble_model,new_tree_model,decision_tree_model,new_model_input_data,branches_predictions):\n",
    "    \"\"\"\n",
    "    In this function we build a comparison dataframe. we document the results for the test set\n",
    "    \"\"\"\n",
    "    comparison_df=pd.DataFrame(test_x)\n",
    "    comparison_df['ensmble_predictions']=ensemble_model.predict(test_x)\n",
    "    new_model_predictions,new_model_depth,new_model_probas = get_new_model_predictions(new_tree_model,test_x,new_model_input_data)\n",
    "    output_dict['new_model_probas']=np.array([list(i) for i in new_model_probas])\n",
    "    output_dict['ensemble_probas']=ensemble_model.predict_proba(test_x)\n",
    "    comparison_df['new_tree_predictions']=new_model_predictions\n",
    "    comparison_df['new_model_depth']=new_model_depth\n",
    "    output_dict['decision_tree_depth']=np.log2(len(decision_tree_model.tree_.n_node_samples))\n",
    "    comparison_df['actual']=list(test_y)\n",
    "    if len(branches_predictions)==len(comparison_df):\n",
    "        comparison_df['branches_predictions']=branches_predictions\n",
    "    comparison_df['decision_tree_predictions']=decision_tree_model.predict(test_x)\n",
    "    instances_depth=[]\n",
    "    new_model_depth=[]\n",
    "    for i in test_x:\n",
    "        temp_depth=[]\n",
    "        for m in ensemble_model.estimators_:\n",
    "            temp_depth.append(np.round(np.log2(m.apply(i)[0])))\n",
    "        instances_depth.append(np.sum(temp_depth))\n",
    "    comparison_df['ensemble_depth']=instances_depth\n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tree conversion functions\n",
    "These functions execute a procedure that isnot included in the paper in which we convert the tree nodes from being binary to numeric based on the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_new_features_and_new_thresholds(t,new_model_input_data):\n",
    "    f_names=[col for col in new_model_input_data.columns if 'x' in col]\n",
    "    new_features=[]\n",
    "    new_thresholds=[]\n",
    "    for feature in t.feature:\n",
    "        if feature==-2:\n",
    "            new_features.append(feature)\n",
    "            new_thresholds.append(-2)\n",
    "        else:\n",
    "            new_f,new_t=get_new_node(feature,f_names)\n",
    "            new_features.append(new_f)\n",
    "            new_thresholds.append(new_t)\n",
    "    return new_features,new_thresholds\n",
    "def get_new_model_predictions(new_model,test_x,new_model_input_data):\n",
    "    new_model_predictions=[]\n",
    "    new_model_depth=[]\n",
    "    new_model_probas=[]\n",
    "    t=new_model.tree_\n",
    "    new_features,new_thresholds=get_new_features_and_new_thresholds(t,new_model_input_data)\n",
    "    for inst in test_x:\n",
    "        d=0\n",
    "        i=0\n",
    "        while new_features[i]!=-2:\n",
    "            d+=1\n",
    "            if inst[new_features[i]]>new_thresholds[i]:\n",
    "                i=t.children_left[i]\n",
    "            else:\n",
    "                i=t.children_right[i]\n",
    "        new_model_depth.append(d)\n",
    "        new_model_predictions.append(np.argmax(t.value[i]))\n",
    "        new_model_probas.append(t.value[i][0])\n",
    "    #new_model_predictions=[4 if i else 2 for i in new_model_predictions]\n",
    "    return new_model_predictions,new_model_depth,new_model_probas\n",
    "def get_new_node(feature,f_names):\n",
    "    new_f=int(f_names[feature].split(\"x\")[1].split(']')[0])\n",
    "    new_t=float(f_names[feature].split(' ')[2])\n",
    "    return new_f,new_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment\n",
    "The following code runs the experiment that is described in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nTo use for running the experiment\\nday_string=\\'10_4_17\\'\\nrun_experiment(\\'iris\\',10,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'iris\\',20,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'breast_cancer\\',7,day_string)\\nrun_experiment(\\'breast_cancer\\',10,day_string)\\nrun_experiment(\\'winery\\',7,day_string)\\nrun_experiment(\\'winery\\',10,day_string)\\nrun_experiment(\\'aust_credit\\',7,day_string)\\nrun_experiment(\\'aust_credit\\',10,day_string)\\nrun_experiment(\\'nurse\\',7,day_string)\\nrun_experiment(\\'nurse\\',10,day_string)\\nrun_experiment(\\'diabetes\\',7,day_string)\\nrun_experiment(\\'diabetes\\',10,day_string)\\nrun_experiment(\\'zoo\\',10,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'zoo\\',20,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'balance_scale\\',10,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'balance_scale\\',20,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'transfusion\\',10,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'transfusion\\',20,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'kohkiloyeh\\',10,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'kohkiloyeh\\',7,day_string,max_depth=10,min_leaf_samples=1)\\nrun_experiment(\\'haberman\\',7,day_string)\\nrun_experiment(\\'haberman\\',10,day_string)\\nrun_experiment(\\'user_modelling\\',10,day_string)\\nrun_experiment(\\'user_modelling\\',20,day_string,max_depth=10,min_leaf_samples=1)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_data_set(s):\n",
    "    \"\"\"\n",
    "    This is sort of configuration function\n",
    "    The function is called for every dataset that we want to test in our experiment. 's' is the\n",
    "    name of the dataset. according to 's' we create a dataset (manually configured)\n",
    "    It returns the features matrix (X) and class vector for every dataset \n",
    "    \"\"\"\n",
    "    if s=='breast_cancer':\n",
    "        names=['code_number','Clump_thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size'\n",
    "            ,'Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','class']\n",
    "        data=pd.read_csv(DATASETS_PATH+'breast-cancer-wisconsin.data',names=names)\n",
    "        data=data[data['Bare Nuclei']!='?']\n",
    "        data['Bare Nuclei']=[int(i) for i in data['Bare Nuclei']]\n",
    "        data['class']=[0 if i==2 else 1 for i in data['class']]\n",
    "    if s=='iris':\n",
    "        iris = load_iris()\n",
    "        data = pd.DataFrame(iris.data[:],columns=iris.feature_names)\n",
    "        data['class'] = iris.target\n",
    "    if s=='winery':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"wine.data\",names=['class','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium',\n",
    "                                     'Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins',\n",
    "                                    'Color intensity','Hue','OD280/OD315 of diluted wines','Proline'])\n",
    "        data['class']=[i-1 for i in data['class']]\n",
    "    if s=='vehicle':\n",
    "        column_names=[str(i) for i in xrange(19)]\n",
    "        column_names.append('class')\n",
    "        pathes=glob.glob(DATASETS_PATH+\"vehicle_data/xa*.dat\")\n",
    "        data=pd.read_csv(pathes[0],sep=\" \",names=column_names)\n",
    "        for p in pathes[1:]:\n",
    "            data=data.append(pd.read_csv(p,sep=\" \",names=column_names),ignore_index=True)\n",
    "        data['class']=data['18']\n",
    "        data=data.drop(['18'],axis=1)\n",
    "        new_classes={v:k for k,v in enumerate(list(set(data['class'].values)))}\n",
    "        data['class']=[new_classes[i] for i in data['class']]\n",
    "    if s=='car':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"car.data\",names=['buying','maint','doors','persons','lug_boot','safety','class'])\n",
    "        for col in data.columns[:-1]:\n",
    "            temp_df=pd.get_dummies(data[col])\n",
    "            temp_df.columns=[col+\"_\"+val for val in temp_df.columns]\n",
    "            data=data.join(temp_df)\n",
    "            data=data.drop([col],axis=1)\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='glass':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"glass.data\",names=['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe','class'])\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='aust_credit':\n",
    "        names=[\"A\"+str(i) for i in range(1,15)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(DATASETS_PATH+\"australian.dat\",sep=\" \",names=names)\n",
    "    if s=='nurse':\n",
    "        names=['x'+str(i) for i in range(1,9)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(DATASETS_PATH+\"post-operative.data\",names=names)\n",
    "        for col in data.columns[:-1]:\n",
    "            temp_df=pd.get_dummies(data[col])\n",
    "            temp_df.columns=[col+\"_\"+val for val in temp_df.columns]\n",
    "            data=data.join(temp_df)\n",
    "            data=data.drop([col],axis=1)\n",
    "        new_classes={v:k for k,v in enumerate(list(set(data['class'].values)))}\n",
    "        data['class']=[new_classes[i] for i in data['class']]\n",
    "    if s=='diabetes':\n",
    "        names=['x'+str(i) for i in range(1,9)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(DATASETS_PATH+\"pima-indians-diabetes.data\",names=names)\n",
    "    if s=='monk1':\n",
    "        names=['class']\n",
    "        names.extend(['x'+str(i) for i in range(1,8)])\n",
    "        data=pd.read_csv(DATASETS_PATH+\"monks-1.train\",sep=\" \",names=names)\n",
    "        data=data.append(pd.read_csv(DATASETS_PATH+\"monks-1.test\",sep=\" \",names=names))\n",
    "        data.index=np.arange(len(data))\n",
    "        data=data.drop(['x7'],axis=1)\n",
    "    if s=='monk2':\n",
    "        names=['class']\n",
    "        names.extend(['x'+str(i) for i in range(1,8)])\n",
    "        data=pd.read_csv(DATASETS_PATH+\"monks-2.train\",sep=\" \",names=names)\n",
    "        data=data.append(pd.read_csv(DATASETS_PATH+\"monks-2.test\",sep=\" \",names=names))\n",
    "        data.index=np.arange(len(data))\n",
    "        data=data.drop(['x7'],axis=1)\n",
    "    if s=='monk3':\n",
    "        names=['class']\n",
    "        names.extend(['x'+str(i) for i in range(1,8)])\n",
    "        data=pd.read_csv(DATASETS_PATH+\"monks-3.train\",sep=\" \",names=names)\n",
    "        data=data.append(pd.read_csv(DATASETS_PATH+\"monks-3.test\",sep=\" \",names=names))\n",
    "        data.index=np.arange(len(data))\n",
    "        data=data.drop(['x7'],axis=1)\n",
    "    if s=='zoo':\n",
    "        names=['x'+str(i) for i in range(0,17)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(DATASETS_PATH+'zoo.data',names=names)\n",
    "        data=data.drop(['x0'],axis=1)\n",
    "        data['class']=[i-1 for i in data['class']]\n",
    "    \n",
    "    if s=='tic_tac_toe':\n",
    "        names=[\"x\"+str(i) for i in range(1,10)]\n",
    "        names.append('class')\n",
    "        data=pd.read_csv(\"/home/sagio/Unitree forest/datasets/tic-tac-toe.data\",names=names)\n",
    "        for col in data.columns[:-1]:\n",
    "            temp_df=pd.get_dummies(data[col])\n",
    "            temp_df.columns=[col+\"_\"+val for val in temp_df.columns]\n",
    "            data=data.join(temp_df)\n",
    "            data=data.drop([col],axis=1)\n",
    "        data['class']=[1 if i=='positive' else 0 for i in data['class']]\n",
    "    if s=='letter':\n",
    "        names=['class']\n",
    "        names.extend(['x'+str(i) for i in xrange(1,17)])\n",
    "        data=pd.read_csv(DATASETS_PATH+\"letter-recognition.data\",names=names)\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='balance_scale':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"balance-scale.data\",names=['class','x1','x2','x3','x4'])\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='ecoli':\n",
    "        f=open(DATASETS_PATH+\"ecoli.data\")\n",
    "        line=f.readline()\n",
    "        l=[]\n",
    "        names=['x'+str(i) for i in range(1,9)]\n",
    "        names.append('class')\n",
    "        while line:\n",
    "            line=line.replace(\"\\n\",\"\").replace(\"    \",\"   \").replace(\"   \",\"  \").replace(\"  \",\" \").split(\" \")\n",
    "            l.append({k:v for k,v in zip(names,line)})\n",
    "            line=f.readline()\n",
    "        data=pd.DataFrame(l)\n",
    "        data=data.drop(['x1'],axis=1)\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='transfusion':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"transfusion.data\")\n",
    "        data['class']=data['whether he/she donated blood in March 2007']\n",
    "        data=data.drop(['whether he/she donated blood in March 2007'], axis=1)\n",
    "    if s=='user_modelling':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"User_Modeling.csv\")\n",
    "        data['class']=data[' UNS']\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "        data=data.drop([' UNS'],axis=1)\n",
    "    if s=='kohkiloyeh':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"kohkiloyeh.csv\")\n",
    "        data['class']=data['pb']\n",
    "        data=data.drop(['pb'],axis=1)\n",
    "        for col in data.columns[:-1]:\n",
    "            temp_df=pd.get_dummies(data[col])\n",
    "            temp_df.columns=[col+\"_\"+val for val in temp_df.columns]\n",
    "            data=data.join(temp_df)\n",
    "            data=data.drop([col],axis=1)\n",
    "        class_map={v:k for k,v in enumerate(set(data['class']))}\n",
    "        data['class']=[class_map[i] for i in data['class']]\n",
    "    if s=='haberman':\n",
    "        data=pd.read_csv(DATASETS_PATH+\"haberman.data\",names=['x1','x2','x3','class'])\n",
    "        data['class']=[i-1 for i in data['class']]\n",
    "    data=data.sample(frac=1)\n",
    "    X = data.drop(['class'],axis=1)\n",
    "    y = data['class'].values\n",
    "    return X,y\n",
    "def run_experiment(s,number_of_trees,day_string,max_depth=3,min_leaf_samples=10):\n",
    "    \"\"\"\n",
    "    Run experiment runs an experiment 100 times for the given dataset and the given configuration.\n",
    "    configuration also include number of trees, and other hyper-parameters for the ensemble\n",
    "    \"\"\"\n",
    "    for RANDOM_SEED in range(1,100):\n",
    "        global output_dict\n",
    "        output_dict={}\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        dest=PICKLES_PATH+s+\"/\"+day_string+\"_\"+str(number_of_trees)+\"trees\"\n",
    "        if not os.path.isdir(dest):\n",
    "            os.makedirs(dest)\n",
    "        OUTPUT_PATH=dest+\"/seed_\"+str(RANDOM_SEED)+\".pkl\"\n",
    "        if os.path.isfile(OUTPUT_PATH):\n",
    "            continue\n",
    "        X,y=create_data_set(s)\n",
    "        global NUMOFLABELS\n",
    "        global NUMBEROFFEATURES\n",
    "        NUMOFLABELS=len(set(y))\n",
    "        NUMBEROFFEATURES=len(X.columns)\n",
    "        train_x,train_y,test_x,test_y=divide_to_train_test(X,y)\n",
    "        ensemble_model=fit_ensemble_model(train_x,train_y,number_of_trees,max_depth=max_depth,min_leaf_samples=min_leaf_samples)\n",
    "        decision_tree_model=fit_decision_tree_model(train_x,train_y)\n",
    "        branches=build_rules_set(ensemble_model)\n",
    "        branches_predictions=get_branches_predictions(branches,test_x)\n",
    "        new_model_input_data=create_new_model_input(branches)\n",
    "        new_tree_model=fit_new_model(new_model_input_data)\n",
    "        comparison_df=create_output_df(test_x,test_y,ensemble_model,new_tree_model,decision_tree_model,new_model_input_data,branches_predictions)\n",
    "        create_output_dict(OUTPUT_PATH,train_x,train_y,test_x,test_y,ensemble_model,\n",
    "                       decision_tree_model,new_tree_model,comparison_df)\n",
    "        print float(len(comparison_df[comparison_df['actual']==comparison_df['ensmble_predictions']]))/len(comparison_df)\n",
    "        print float(len(comparison_df[comparison_df['actual']==comparison_df['new_tree_predictions']]))/len(comparison_df)\n",
    "        print float(len(comparison_df[comparison_df['actual']==comparison_df['decision_tree_predictions']]))/len(comparison_df)\n",
    "\"\"\"\"\n",
    "To use for running the experiment\n",
    "day_string='10_4_17'\n",
    "run_experiment('iris',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('iris',20,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('breast_cancer',7,day_string)\n",
    "run_experiment('breast_cancer',10,day_string)\n",
    "run_experiment('winery',7,day_string)\n",
    "run_experiment('winery',10,day_string)\n",
    "run_experiment('aust_credit',7,day_string)\n",
    "run_experiment('aust_credit',10,day_string)\n",
    "run_experiment('nurse',7,day_string)\n",
    "run_experiment('nurse',10,day_string)\n",
    "run_experiment('diabetes',7,day_string)\n",
    "run_experiment('diabetes',10,day_string)\n",
    "run_experiment('zoo',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('zoo',20,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('balance_scale',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('balance_scale',20,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('transfusion',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('transfusion',20,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('kohkiloyeh',10,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('kohkiloyeh',7,day_string,max_depth=10,min_leaf_samples=1)\n",
    "run_experiment('haberman',7,day_string)\n",
    "run_experiment('haberman',10,day_string)\n",
    "run_experiment('user_modelling',10,day_string)\n",
    "run_experiment('user_modelling',20,day_string,max_depth=10,min_leaf_samples=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An ilustrative example - Iris dataset\n",
    "The following cells present a single run for the Iris dataset. We build ensemble of 20 trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s='iris'\n",
    "X,y=create_data_set(s)\n",
    "NUMOFLABELS=len(set(y))\n",
    "NUMBEROFFEATURES=len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>6.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "131                7.9               3.8                6.4               2.0\n",
       "85                 6.0               3.4                4.5               1.6\n",
       "97                 6.2               2.9                4.3               1.3\n",
       "113                5.7               2.5                5.0               2.0\n",
       "40                 5.0               3.5                1.3               0.3\n",
       "73                 6.1               2.8                4.7               1.2\n",
       "147                6.5               3.0                5.2               2.0\n",
       "22                 4.6               3.6                1.0               0.2\n",
       "138                6.0               3.0                4.8               1.8\n",
       "88                 5.6               3.0                4.1               1.3\n",
       "111                6.4               2.7                5.3               1.9\n",
       "89                 5.5               2.5                4.0               1.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "79                 5.7               2.6                3.5               1.0\n",
       "81                 5.5               2.4                3.7               1.0\n",
       "58                 6.6               2.9                4.6               1.3\n",
       "23                 5.1               3.3                1.7               0.5\n",
       "145                6.7               3.0                5.2               2.3\n",
       "24                 4.8               3.4                1.9               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "64                 5.6               2.9                3.6               1.3\n",
       "44                 5.1               3.8                1.9               0.4\n",
       "29                 4.7               3.2                1.6               0.2\n",
       "7                  5.0               3.4                1.5               0.2\n",
       "112                6.8               3.0                5.5               2.1\n",
       "141                6.9               3.1                5.1               2.3\n",
       "120                6.9               3.2                5.7               2.3\n",
       "123                6.3               2.7                4.9               1.8\n",
       "..                 ...               ...                ...               ...\n",
       "11                 4.8               3.4                1.6               0.2\n",
       "27                 5.2               3.5                1.5               0.2\n",
       "91                 6.1               3.0                4.6               1.4\n",
       "134                6.1               2.6                5.6               1.4\n",
       "127                6.1               3.0                4.9               1.8\n",
       "100                6.3               3.3                6.0               2.5\n",
       "33                 5.5               4.2                1.4               0.2\n",
       "56                 6.3               3.3                4.7               1.6\n",
       "143                6.8               3.2                5.9               2.3\n",
       "133                6.3               2.8                5.1               1.5\n",
       "99                 5.7               2.8                4.1               1.3\n",
       "110                6.5               3.2                5.1               2.0\n",
       "142                5.8               2.7                5.1               1.9\n",
       "59                 5.2               2.7                3.9               1.4\n",
       "149                5.9               3.0                5.1               1.8\n",
       "69                 5.6               2.5                3.9               1.1\n",
       "52                 6.9               3.1                4.9               1.5\n",
       "60                 5.0               2.0                3.5               1.0\n",
       "93                 5.0               2.3                3.3               1.0\n",
       "140                6.7               3.1                5.6               2.4\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "139                6.9               3.1                5.4               2.1\n",
       "72                 6.3               2.5                4.9               1.5\n",
       "126                6.2               2.8                4.8               1.8\n",
       "62                 6.0               2.2                4.0               1.0\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "5                  5.4               3.9                1.7               0.4\n",
       "36                 5.5               3.5                1.3               0.2\n",
       "26                 5.0               3.4                1.6               0.4\n",
       "65                 6.7               3.1                4.4               1.4\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 2, 0, 1, 2, 0, 2, 1, 2, 1, 2, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1,\n",
       "       0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 2, 1, 1, 2, 1, 0, 2, 2, 2, 1, 0, 0, 2,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2, 2, 0, 2, 0, 0, 1, 1, 0, 0, 1, 2,\n",
       "       1, 0, 0, 1, 1, 2, 2, 1, 1, 1, 0, 0, 2, 0, 1, 1, 2, 2, 1, 1, 0, 0, 0,\n",
       "       1, 2, 2, 2, 0, 1, 2, 1, 0, 2, 2, 0, 2, 0, 2, 1, 0, 2, 1, 0, 0, 2, 0,\n",
       "       2, 1, 1, 2, 1, 0, 0, 1, 2, 2, 2, 0, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1,\n",
       "       1, 2, 0, 2, 1, 2, 1, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x,train_y,test_x,test_y=divide_to_train_test(X,y)\n",
    "ensemble_model=fit_ensemble_model(train_x,train_y,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of branches (rules-sets): 2192\n"
     ]
    }
   ],
   "source": [
    "branches=build_rules_set(ensemble_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] <4.95, [2] <1.45, [3] <0.75, {0: 19.687840290381125, 1: 0.20689655172413793, 2: 0.10526315789473684}\n",
      "[3] >0.751, [0] <4.95, [2] <1.45, [3] <0.8, {0: 18.687840290381125, 1: 1.2068965517241379, 2: 0.10526315789473684}\n",
      "[3] >0.801, [0] <4.95, [2] <1.45, [3] <1.35, {0: 12.687840290381125, 1: 7.1443965517241379, 2: 0.16776315789473684}\n",
      "[3] >1.351, [0] <4.95, [2] <1.45, [3] <1.45, {0: 12.687840290381125, 1: 6.909102434077079, 2: 0.40305727554179566}\n",
      "[3] >1.451, [0] <4.95, [2] <1.45, [3] <1.55, {0: 12.687840290381125, 1: 6.6233881483627934, 2: 0.68877156125608141}\n",
      "[3] >1.551, [0] <4.95, [2] <1.45, [3] <1.65, {0: 12.687840290381125, 1: 5.973388148362794, 2: 1.3387715612560815}\n",
      "[3] >1.651, [0] <4.95, [2] <1.45, [3] <1.75, {0: 12.687840290381125, 1: 4.9733881483627931, 2: 2.3387715612560815}\n",
      "[3] >1.751, [0] <4.95, [2] <1.45, {0: 12.687840290381125, 1: 3.3179259634888436, 2: 3.994233746130031}\n",
      "[0] >4.951, [0] <5.45, [2] <1.45, [3] <0.75, {0: 19.737547892720308, 1: 0.26245210727969348, 2: 0.0}\n",
      "[0] >4.951, [3] >0.751, [0] <5.45, [2] <1.45, [3] <0.8, {0: 18.737547892720308, 1: 1.2624521072796935, 2: 0.0}\n",
      "[0] >4.951, [3] >0.801, [0] <5.45, [2] <1.45, [3] <1.35, {0: 12.737547892720308, 1: 7.1999521072796933, 2: 0.0625}\n",
      "[0] >4.951, [3] >1.351, [0] <5.45, [2] <1.45, [3] <1.45, {0: 12.737547892720308, 1: 6.9646579896326344, 2: 0.29779411764705882}\n",
      "[0] >4.951, [3] >1.451, [0] <5.45, [2] <1.45, [3] <1.55, {0: 12.737547892720308, 1: 6.6789437039183488, 2: 0.58350840336134446}\n",
      "[0] >4.951, [3] >1.551, [0] <5.45, [2] <1.45, [3] <1.65, {0: 12.737547892720308, 1: 6.0289437039183493, 2: 1.2335084033613444}\n",
      "[0] >4.951, [3] >1.651, [0] <5.45, [2] <1.45, [3] <1.75, {0: 12.737547892720308, 1: 5.0289437039183484, 2: 2.2335084033613444}\n",
      "[0] >4.951, [3] >1.751, [0] <5.45, [2] <1.45, {0: 12.737547892720308, 1: 3.373481519044399, 2: 3.8889705882352938}\n",
      "[2] >1.451, [0] <4.95, [2] <2.35, [3] <0.75, {0: 19.021173623714457, 1: 0.66522988505747127, 2: 0.31359649122807021}\n",
      "[2] >2.351, [0] <4.95, [2] <2.45, [3] <0.75, {0: 18.021173623714457, 1: 1.6652298850574714, 2: 0.31359649122807021}\n",
      "[2] >1.451, [3] >0.751, [0] <4.95, [2] <2.35, [3] <0.8, {0: 18.021173623714457, 1: 1.6652298850574714, 2: 0.31359649122807021}\n",
      "[2] >2.351, [3] >0.751, [0] <4.95, [2] <2.45, [3] <0.8, {0: 17.021173623714457, 1: 2.6652298850574714, 2: 0.31359649122807021}\n"
     ]
    }
   ],
   "source": [
    "print_branches(branches[:20]) # print first 20 rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare branches predictions with ensemble predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in test set: 45\n",
      "Number of agreed predictions: 45\n"
     ]
    }
   ],
   "source": [
    "branches_predictions=get_branches_predictions(branches,test_x)\n",
    "agreed_predictions_counter=0\n",
    "for ensemble_pred,branches_pred in zip(ensemble_model.predict(test_x),branches_predictions):\n",
    "    if ensemble_pred==branches_pred:\n",
    "        agreed_predictions_counter+=1\n",
    "print \"Number of observations in test set: \"+str(len(branches_predictions))\n",
    "print \"Number of agreed predictions: \"+str(agreed_predictions_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>ensmble_predictions</th>\n",
       "      <th>new_tree_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual  ensmble_predictions  new_tree_predictions\n",
       "0        0                    0                     0\n",
       "1        2                    2                     2\n",
       "2        1                    1                     1\n",
       "3        0                    0                     0\n",
       "4        2                    2                     2\n",
       "5        1                    1                     1\n",
       "6        0                    0                     0\n",
       "7        0                    0                     0\n",
       "8        2                    2                     2\n",
       "9        0                    0                     0\n",
       "10       2                    2                     2\n",
       "11       1                    1                     1\n",
       "12       1                    1                     1\n",
       "13       2                    2                     2\n",
       "14       1                    1                     1\n",
       "15       0                    0                     0\n",
       "16       0                    0                     0\n",
       "17       1                    1                     1\n",
       "18       2                    1                     1\n",
       "19       2                    2                     2\n",
       "20       2                    2                     2\n",
       "21       0                    0                     0\n",
       "22       1                    1                     1\n",
       "23       2                    2                     2\n",
       "24       2                    1                     1\n",
       "25       1                    1                     1\n",
       "26       2                    2                     2\n",
       "27       2                    2                     2\n",
       "28       1                    1                     1\n",
       "29       2                    2                     2\n",
       "30       1                    1                     1\n",
       "31       1                    1                     1\n",
       "32       1                    1                     1\n",
       "33       1                    1                     1\n",
       "34       2                    2                     2\n",
       "35       0                    0                     0\n",
       "36       2                    2                     2\n",
       "37       1                    1                     1\n",
       "38       2                    1                     1\n",
       "39       1                    1                     1\n",
       "40       0                    0                     0\n",
       "41       0                    0                     0\n",
       "42       0                    0                     0\n",
       "43       0                    0                     0\n",
       "44       1                    1                     1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict={}\n",
    "new_model_input_data=create_new_model_input(branches)\n",
    "decision_tree_model=fit_decision_tree_model(train_x,train_y)\n",
    "new_tree_model=fit_new_model(new_model_input_data)\n",
    "comparison_df=create_output_df(test_x,test_y,ensemble_model,new_tree_model,decision_tree_model,new_model_input_data,branches_predictions)\n",
    "comparison_df[['actual','ensmble_predictions','new_tree_predictions']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
